{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision and Pattern Recognition - Project 3 (CNN classifier)\n",
    "#### Gaia Marsich [SM3500600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introduction](#intro)\n",
    "* [1. Task 1](#1-bullet)\n",
    "* [2. Task 2](#2-bullet)\n",
    "* [3. Task 3](#3-bullet)\n",
    "* [References](#ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a class=\"anchor\" id=\"intro\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project requires the implementation of an image classifier based on convolutional neural networks. The provided dataset (from [Lazebnik et al., 2006]), contains 15 categories (office, kitchen, living room, bedroom, store, industrial, tall building, inside city, street, highway, coast, open country, mountain, forest, suburb), and is already divided in training set and test set.\n",
    "\n",
    "First of all, let's do the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageOps\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models.alexnet import AlexNet_Weights\n",
    "\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from side_code import CNN\n",
    "from side_code import train_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 <a class=\"anchor\" id=\"1-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1da7c11e2f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set a seed for reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "train_path = '/Users/Gaia/Desktop/CVPR-project/CVPR-project/data/train'\n",
    "test_path = '/Users/Gaia/Desktop/CVPR-project/CVPR-project/data/test'\n",
    "\n",
    "train = ImageFolder(root=train_path, transform=train_test.transform)\n",
    "dataset_test = ImageFolder(root=test_path, transform=train_test.transform)\n",
    "\n",
    "\n",
    "# Split the provided training set in 85% for training set and 15% for validation set\n",
    "\n",
    "train_size = int(0.85 * len(train))\n",
    "val_size = len(train) - train_size\n",
    "\n",
    "dataset_train, dataset_val = random_split(train, [train_size, val_size])\n",
    "\n",
    "\n",
    "# Create the loaders\n",
    "\n",
    "batch_size = 32 # batch_size = 32 as required by the project\n",
    "train_loader = DataLoader(dataset_train, batch_size, shuffle=True) \n",
    "val_loader = DataLoader(dataset_val, batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset_test, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate a model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a model\n",
    "\n",
    "model = CNN.CNN1()\n",
    "\n",
    "# Set parameters for the training\n",
    "\n",
    "learning_rate = 0.0005\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), momentum=0.9, lr=learning_rate) # the momentum by default is 0, but I need it different from 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and do the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the training\n",
    "model_path, loss_train, loss_val, accuracies_val = train_test.train_model(model = model, EPOCHS = 40, train_loader = train_loader, val_loader = val_loader, optimizer = optimizer, loss_function = loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the training is performed with the support of a validation set so that, during the training, the generalisation capability of the current network can be estimated. The analysis keeps going until the number of epochs that has been set is reached.\n",
    "However, the algorithm compares the model at the current epoch with the best model previously obtained, where the performances are based on the validation loss. At the end, the more recent model will be the optimal one among those that had been tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the plot of the loss during the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(1, len(loss_train) + 1)) # epochs started from 0\n",
    "\n",
    "plt.plot(epochs, loss_train, label='Training loss')\n",
    "plt.plot(epochs, loss_val, label='Validation loss')\n",
    "plt.title('Training and validation loss over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, since the network is trained on a specific train set over and over, the loss associated to this set keeps decreasing. On the other hand, the tests done on a separate set, the validation set, report an increase of the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it is the plot on the validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, accuracies_val, color='orange', label='Validation accuracy')\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy [%]')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more the network is trained the more it is expected to perform correctly. In this graph, one can observe that the accuracy associated to the validation set increases; however, it shows an oscillating trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the accuracy on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained network on the test set, and collect the infos for the confusion matrix\n",
    "\n",
    "train_test.perform_test(model = model, model_path = model_path, test_loader = test_loader, dataset_test = dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 <a class=\"anchor\" id=\"2-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous result can be improved using some techniques. Here, I will use:\n",
    "\n",
    "* data augmentation: the only difference with the original version is the train/validation dataset: \"new\" images are added to the initial training and validation dataset. The \"new\" images are simply the left-to-right reflections of the original train/validation dataset.\n",
    "* batch normalization\n",
    "* different size of convolutional filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remark that from Task 1 we already have dataset_train, dataset_val and dataset_test\n",
    "\n",
    "# Data augmentation for the train dataset\n",
    "\n",
    "flipped_dataset_train = train_test.HorizontallyFlippedDataset(dataset_train)\n",
    "data_augmentation_dataset_train = torch.utils.data.ConcatDataset([dataset_train, flipped_dataset_train])\n",
    "\n",
    "\n",
    "# Create the loaders\n",
    "\n",
    "batch_size = 32\n",
    "data_augmentation_train_loader = DataLoader(data_augmentation_dataset_train, batch_size, shuffle=True) \n",
    "val_loader = DataLoader(dataset_val, batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset_test, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's instantiate the model and perform the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a model\n",
    "\n",
    "model_data_augmentation = CNN.CNN2()\n",
    "\n",
    "\n",
    "# Set parameters for the training\n",
    "\n",
    "learning_rate = 0.0005\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_data_augmentation.parameters(), momentum=0.9, lr=learning_rate) # the momentum by default is 0, but I need it different from 0\n",
    "\n",
    "# Perform the training\n",
    "task2_model_path, task2_loss_train, task2_loss_val, task2_accuracies_val = train_test.train_model(model = model_data_augmentation, EPOCHS = 40, train_loader = data_augmentation_train_loader, val_loader = val_loader, optimizer = optimizer, loss_function = loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained network on the test set\n",
    "\n",
    "train_test.perform_test(model = model_data_augmentation, model_path = task2_model_path, test_loader = test_loader, dataset_test = dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will investigate what happens also using:\n",
    "\n",
    "* Adam optimizer\n",
    "* dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a model\n",
    "\n",
    "model_data_augmentation_1 = CNN.CNN3()\n",
    "\n",
    "\n",
    "# Set parameters for the training\n",
    "\n",
    "learning_rate = 0.0005\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_data_augmentation_1.parameters(), lr=learning_rate) # the momentum by default is 0, but I need it different from 0\n",
    "\n",
    "# Perform the training\n",
    "task2_1_model_path, task2_1_loss_train, task2_1_loss_val, task2_1_accuracies_val = train_test.train_model(model = model_data_augmentation_1, EPOCHS = 40, train_loader = data_augmentation_train_loader, val_loader = val_loader, optimizer = optimizer, loss_function = loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained network on the test set\n",
    "\n",
    "train_test.perform_test(model = model_data_augmentation_1, model_path = task2_1_model_path, test_loader = test_loader, dataset_test = dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 <a class=\"anchor\" id=\"3-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the transfer learning based on a pre-trained network (here, AlexNet in particular) is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alexnet (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights of all the layers but the last fully connected layer will be frozen and the weights of the last layer will be fine-tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset; remark that AlexNet requires 3 channels as input (so, RGB)\n",
    "\n",
    "train_path = '/Users/Gaia/Desktop/CVPR-project/CVPR-project/data/train'\n",
    "test_path = '/Users/Gaia/Desktop/CVPR-project/CVPR-project/data/test'\n",
    "\n",
    "train_ALEXNET = ImageFolder(root=train_path, transform=train_test.transform_ALEXNET)\n",
    "dataset_test_ALEXNET = ImageFolder(root=test_path, transform=train_test.transform_ALEXNET)\n",
    "\n",
    "\n",
    "# Split the provided training set in 85% for training set and 15% for validation set\n",
    "\n",
    "train_size_ALEXNET = int(0.85 * len(train_ALEXNET))\n",
    "val_size_ALEXNET = len(train_ALEXNET) - train_size_ALEXNET\n",
    "\n",
    "dataset_train_ALEXNET, dataset_val_ALEXNET = random_split(train_ALEXNET, [train_size_ALEXNET, val_size_ALEXNET])\n",
    "\n",
    "# Data augmentation\n",
    "\n",
    "flipped_dataset_train_ALEXNET = train_test.HorizontallyFlippedDataset(dataset_train_ALEXNET)\n",
    "data_augmentation_dataset_train_ALEXNET = torch.utils.data.ConcatDataset([dataset_train_ALEXNET, flipped_dataset_train_ALEXNET])\n",
    "\n",
    "\n",
    "# Create the loaders\n",
    "\n",
    "batch_size = 32\n",
    "data_augmentation_train_loader_ALEXNET = DataLoader(data_augmentation_dataset_train_ALEXNET, batch_size, shuffle=True) \n",
    "val_loader_ALEXNET = DataLoader(dataset_val_ALEXNET, batch_size, shuffle=False)\n",
    "test_loader_ALEXNET = DataLoader(dataset_test_ALEXNET, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = models.alexnet(weights=AlexNet_Weights.DEFAULT) # weights=AlexNet_Weights.DEFAULT instead of pretrained=True as suggested by a warning\n",
    "\n",
    "for param in alexnet.parameters(): # freeze all layers except the last fully connected layer\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "in_features = alexnet.classifier[6].in_features # modify the last fully connected layer for your task\n",
    "alexnet.classifier[6] = nn.Linear(in_features, 15)\n",
    "\n",
    "for param in alexnet.classifier[6].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(alexnet.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "task3_model_path, task3_loss_train, task3_loss_val, task3_accuracies_val = train_test.train_model(model = alexnet, EPOCHS = 8, train_loader = data_augmentation_train_loader_ALEXNET, val_loader = val_loader_ALEXNET, optimizer = optimizer, loss_function = loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.perform_test(model = alexnet, model_path = task3_model_path, test_loader = test_loader_ALEXNET, dataset_test = dataset_test_ALEXNET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alexnet (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-trained network will be employed as a feature extractor, accessing the activation of an intermediate layer (for instance, one of the fully connected layers); a multiclass linear SVM will be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset; remark that AlexNet requires 3 channels as input (so, RGB)\n",
    "\n",
    "train_path = '/Users/Gaia/Desktop/CVPR-project/CVPR-project/data/train'\n",
    "test_path = '/Users/Gaia/Desktop/CVPR-project/CVPR-project/data/test'\n",
    "\n",
    "dataset_train_ALEXNET_2 = ImageFolder(root=train_path, transform=train_test.transform_ALEXNET)\n",
    "dataset_test_ALEXNET_2 = ImageFolder(root=test_path, transform=train_test.transform_ALEXNET)\n",
    "\n",
    "\n",
    "# Data augmentation\n",
    "\n",
    "flipped_dataset_train_ALEXNET_2 = train_test.HorizontallyFlippedDataset(dataset_train_ALEXNET_2)\n",
    "data_augmentation_dataset_train_ALEXNET_2 = torch.utils.data.ConcatDataset([dataset_train_ALEXNET_2, flipped_dataset_train_ALEXNET_2])\n",
    "\n",
    "\n",
    "# Create the loaders\n",
    "\n",
    "batch_size = 32\n",
    "data_augmentation_train_loader_ALEXNET_2 = DataLoader(data_augmentation_dataset_train_ALEXNET_2, batch_size, shuffle=True) \n",
    "test_loader_ALEXNET_2 = DataLoader(dataset_test_ALEXNET_2, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gaia\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 79.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gaia\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "alexnet = models.alexnet(weights=AlexNet_Weights.DEFAULT)\n",
    "\n",
    "for param in alexnet.parameters(): # freezes the parameters of the pre-trained model; these parameters will not be updated during the training process\n",
    "    param.requires_grad = False\n",
    "\n",
    "train_features, train_labels = train_test.extract(alexnet, data_augmentation_train_loader_ALEXNET_2) #TOFO: dunque davvero non serve il val?\n",
    "test_features, test_labels = train_test.extract(alexnet, test_loader_ALEXNET_2)\n",
    "\n",
    "\n",
    "# Train a multiclass linear SVM\n",
    "\n",
    "SVM = svm.LinearSVC(C=1.0, max_iter=1000) #TODO: forse modificare i parametri #TODO: che approccio esattamente Ã¨ stato usato? La DAG?\n",
    "SVM.fit(train_features, train_labels)\n",
    "\n",
    "\n",
    "# Evaluate the SVM on the test set\n",
    "\n",
    "y_pred = SVM.predict(test_features)\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References <a class=\"anchor\" id=\"ref\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://dingyan89.medium.com/calculating-parameters-of-convolutional-and-fully-connected-layers-with-keras-186590df36c6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
