{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision and Pattern Recognition - Project 3 (CNN classifier)\n",
    "#### Gaia Marsich [SM3500600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introduction](#intro)\n",
    "* [1. Task 1](#1-bullet)\n",
    "* [2. Task 2](#2-bullet)\n",
    "* [3. Task 3](#3-bullet)\n",
    "* [References](#ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a class=\"anchor\" id=\"#intro\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project requires the implementation of an image classifier based on convolutional neural networks. The provided dataset (from [Lazebnik et al., 2006]), contains 15 categories (office, kitchen, living room, bedroom, store, industrial, tall building, inside city, street, highway, coast, open country, mountain, forest, suburb), and is already divided in training set and test set.\n",
    "\n",
    "First of all, let's do the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 <a class=\"anchor\" id=\"#1-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Build the network\n",
    "\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    # A model will have an __init__() function, where it instantiates its layers\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__() # the constructor of the parent class (nn.Module) is called to initialize the model properly.\n",
    "\n",
    "        # Convolutional layer 1:\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1) # from [1] we get the formula: output = ((input - kernel_size + 2*padding)/stride) + 1 => 62*62\n",
    "        # ReLU activation after conv1\n",
    "        self.relu1 = nn.ReLU() # output: 62*62 #TODO OK\n",
    "        # Max pooling layer 1\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 31*31 (from 62/2)\n",
    "\n",
    "        # Convolutional layer 2\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1) # from [1] we know that output: 29*29\n",
    "        # ReLU activation after conv2\n",
    "        self.relu2 = nn.ReLU() # output: 29*29 #TODO OK\n",
    "        # Max pooling layer 2\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 14*14 (from the test in dim_images.ipynb)\n",
    "\n",
    "        # Convolutional layer 3\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1) # from [1] we know that output: 12*12\n",
    "        # ReLU activation after conv3  \n",
    "        self.relu3 = nn.ReLU() # output: 12*12\n",
    "\n",
    "        # Fully connected layer. 32: number of channels; 12, 12: height and width of the feature map\n",
    "        self.fc = nn.Linear(32 * 12 * 12, 15) #TODO OK\n",
    "        # Classification layer\n",
    "        #self.output = nn.CrossEntropyLoss() #TODO: ma è giusto da mettere? Al momento è tolto\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def initialize_weights(self):       #TODO: the professor mentioned to avoid normalization, what should I do?\n",
    "        for module in self.modules(): # self.modules() comes from nn.Module; to recursively iterate over all the modules\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                init.normal_(module.weight, mean=0, std=0.01) # initial weights drawn from a Gaussian distribution having a mean of 0 and a standard deviation of 0.01\n",
    "                init.constant_(module.bias, 0) # set the bias to 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # A model will have a forward() function\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = x.view(-1, 32 * 12 * 12)  # flatten the tensor before passing to fully connected layers (the size -1 is inferred from other dimensions)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        #x = self.output(x) #TODO: in caso, da eliminare\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the provided training set in 85% for actual training set and 15% to be used as validation set\n",
    "\n",
    "resized_train_path = '/Users/Gaia/Desktop/CVPR-project/CVPR-project/resized/train'\n",
    "\n",
    "dataset_train = ImageFolder(root=resized_train_path, transform=transforms.ToTensor())\n",
    "\n",
    "train_size = int(0.85 * len(dataset_train))\n",
    "val_size = len(dataset_train) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset_train, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) # batch_size=32 required by the project\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False) # batch_size=32 required by the project\n",
    "\n",
    "\n",
    "# Create the test loader\n",
    "resized_test_path = '/Users/Gaia/Desktop/CVPR-project/CVPR-project/resized/test'\n",
    "\n",
    "dataset_test = ImageFolder(root=resized_test_path, transform=transforms.ToTensor())\n",
    "\n",
    "test_loader = DataLoader(dataset_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate a model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu2): ReLU()\n",
      "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu3): ReLU()\n",
      "  (fc): Linear(in_features=4608, out_features=15, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a model\n",
    "model = CNN()\n",
    "print(model)\n",
    "\n",
    "# Set parameters for the training\n",
    "learning_rate = 0.0002\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), momentum=0.9, lr=learning_rate) # the momentum by default is 0, but I need it different from 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and do the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loop...\n",
      "Epoch [1/5], Loss: 2.708\n",
      "Epoch [2/5], Loss: 2.708\n",
      "Epoch [3/5], Loss: 2.708\n",
      "Epoch [4/5], Loss: 2.708\n",
      "Epoch [5/5], Loss: 2.708\n"
     ]
    }
   ],
   "source": [
    "def train_one_epoch(epoch_index,loader): # to train for just one epoch (one epoch: the network sees the whole training set)\n",
    "    running_loss = 0.\n",
    "\n",
    "    for i, data in enumerate(loader):\n",
    "\n",
    "        inputs, labels = data # get the minibatch\n",
    "\n",
    "        outputs = model(inputs) # forward pass\n",
    "\n",
    "        loss = loss_function(outputs, labels) # compute the loss\n",
    "        running_loss += loss.item() # sum up the loss for the minibatches processed so far\n",
    "\n",
    "        optimizer.zero_grad() # notice that by default, the gradients are accumulated, hence we need to set them to zero\n",
    "        loss.backward() # backward pass\n",
    "        optimizer.step() # update the weights\n",
    "\n",
    "    return running_loss/(i+1) # average loss per minibatch\n",
    "\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 5\n",
    "\n",
    "print('Training loop...')\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_one_epoch(epoch,train_loader)\n",
    "    print(f'Epoch [{epoch + 1}/{EPOCHS}], Loss: {train_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to train for just one epoch\n",
    "\n",
    "def train_one_epoch(epoch_index, loader):\n",
    "    running_loss = 0.\n",
    "\n",
    "    for i, data in enumerate(loader):\n",
    "\n",
    "        inputs, labels = data # get the minibatch\n",
    "\n",
    "        outputs = model(inputs) # forward pass\n",
    "\n",
    "        loss = loss_function(outputs, labels) # compute the loss\n",
    "        running_loss += loss.item() # sum up the loss for the minibatches processed so far\n",
    "\n",
    "        optimizer.zero_grad() # notice that by default, the gradients are accumulated, hence we need to set them to zero\n",
    "        loss.backward() # backward pass\n",
    "        optimizer.step() # update the weights\n",
    "\n",
    "    return running_loss/(i+1) # average loss per minibatch\n",
    "\n",
    "\n",
    "\n",
    "# Training\n",
    "\n",
    "EPOCHS = 48\n",
    "\n",
    "best_validation_loss = np.inf\n",
    "\n",
    "loss_train = [] # store the values of the loss for the training set\n",
    "loss_val = [] # store the values of the loss for the validation set\n",
    "accuracies_val = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch+ 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    train_loss = train_one_epoch(epoch, train_loader)\n",
    "\n",
    "    running_validation_loss = 0.0\n",
    "\n",
    "    # If using dropout and/or batch normalization, we need the following, to set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    with torch.no_grad():      # disable gradient computation and reduce memory consumption\n",
    "        for i, vdata in enumerate(val_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_function(voutputs, vlabels)\n",
    "            running_validation_loss += vloss\n",
    "\n",
    "    validation_loss = running_validation_loss / (i + 1) # average validation loss per minibatch\n",
    "    loss_train.append(train_loss)\n",
    "    loss_val.append(validation_loss)\n",
    "    print('LOSS: train: {}; validation: {}'.format(train_loss, validation_loss))\n",
    "\n",
    "\n",
    "    # Track best performance (based on validation), and save the model\n",
    "    if validation_loss < best_validation_loss:\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        best_validation_loss = validation_loss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model and evaluate the performance on the test set\n",
    "# to load the best model, first instantiate a new model, then load its state\n",
    "newModel = CNN()\n",
    "newModel.load_state_dict(torch.load(model_path))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in cifar2_test_loader:\n",
    "        images, labels = data\n",
    "        outputs = newModel(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 <a class=\"anchor\" id=\"#2-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 <a class=\"anchor\" id=\"#3-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References <a class=\"anchor\" id=\"ref\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://dingyan89.medium.com/calculating-parameters-of-convolutional-and-fully-connected-layers-with-keras-186590df36c6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
