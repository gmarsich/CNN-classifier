{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH1:\n",
      "LOSS train: 2.6629636943340302 validation: 2.4765138626098633 | validation_accuracy: 18.666666666666668% \n",
      "EPOCH2:\n",
      "LOSS train: 2.4052781015634537 validation: 2.2252233028411865 | validation_accuracy: 39.55555555555556% \n",
      "EPOCH3:\n",
      "LOSS train: 2.2026075273752213 validation: 2.035402774810791 | validation_accuracy: 49.77777777777778% \n",
      "EPOCH4:\n",
      "LOSS train: 2.0283912286162376 validation: 1.8644353151321411 | validation_accuracy: 55.55555555555556% \n",
      "EPOCH5:\n",
      "LOSS train: 1.8935782179236411 validation: 1.728905439376831 | validation_accuracy: 60.888888888888886% \n",
      "EPOCH6:\n",
      "LOSS train: 1.7894921600818634 validation: 1.6111842393875122 | validation_accuracy: 64.0% \n",
      "EPOCH7:\n",
      "LOSS train: 1.6892506271600722 validation: 1.5114099979400635 | validation_accuracy: 65.33333333333333% \n",
      "EPOCH8:\n",
      "LOSS train: 1.5956883952021599 validation: 1.4263393878936768 | validation_accuracy: 67.11111111111111% \n",
      "EPOCH9:\n",
      "LOSS train: 1.5143862083554267 validation: 1.3539365530014038 | validation_accuracy: 67.11111111111111% \n",
      "EPOCH10:\n",
      "LOSS train: 1.4554661601781844 validation: 1.2917516231536865 | validation_accuracy: 68.0% \n",
      "EPOCH11:\n",
      "LOSS train: 1.4004000753164292 validation: 1.2370492219924927 | validation_accuracy: 68.88888888888889% \n",
      "EPOCH12:\n",
      "LOSS train: 1.3577069833874702 validation: 1.1884419918060303 | validation_accuracy: 69.77777777777777% \n",
      "EPOCH13:\n",
      "LOSS train: 1.3089859887957573 validation: 1.144538402557373 | validation_accuracy: 69.77777777777777% \n",
      "EPOCH14:\n",
      "LOSS train: 1.2639667317271233 validation: 1.1052137613296509 | validation_accuracy: 72.44444444444444% \n",
      "EPOCH15:\n",
      "LOSS train: 1.2168098375201226 validation: 1.0686475038528442 | validation_accuracy: 72.0% \n",
      "EPOCH16:\n",
      "LOSS train: 1.1986692316830159 validation: 1.0363106727600098 | validation_accuracy: 71.55555555555556% \n",
      "EPOCH17:\n",
      "LOSS train: 1.1740342170000075 validation: 1.0082385540008545 | validation_accuracy: 72.0% \n",
      "EPOCH18:\n",
      "LOSS train: 1.1419292613863945 validation: 0.9829229116439819 | validation_accuracy: 72.88888888888889% \n",
      "EPOCH19:\n",
      "LOSS train: 1.1315616734325886 validation: 0.9616155028343201 | validation_accuracy: 72.88888888888889% \n",
      "EPOCH20:\n",
      "LOSS train: 1.0892725251615047 validation: 0.937200665473938 | validation_accuracy: 73.77777777777777% \n",
      "EPOCH21:\n",
      "LOSS train: 1.0702549405395985 validation: 0.9174035787582397 | validation_accuracy: 73.77777777777777% \n",
      "EPOCH22:\n",
      "LOSS train: 1.0504860371351241 validation: 0.8988916873931885 | validation_accuracy: 75.11111111111111% \n",
      "EPOCH23:\n",
      "LOSS train: 1.036071803420782 validation: 0.8811536431312561 | validation_accuracy: 75.11111111111111% \n",
      "EPOCH24:\n",
      "LOSS train: 1.024508885294199 validation: 0.8652690052986145 | validation_accuracy: 75.55555555555556% \n",
      "EPOCH25:\n",
      "LOSS train: 1.005240695923567 validation: 0.8503757119178772 | validation_accuracy: 76.0% \n",
      "EPOCH26:\n",
      "LOSS train: 0.9844793878495693 validation: 0.8370853662490845 | validation_accuracy: 76.0% \n",
      "EPOCH27:\n",
      "LOSS train: 0.9709791801869869 validation: 0.823891818523407 | validation_accuracy: 77.33333333333333% \n",
      "EPOCH28:\n",
      "LOSS train: 0.9570308201014995 validation: 0.8114176392555237 | validation_accuracy: 76.0% \n",
      "EPOCH29:\n",
      "LOSS train: 0.945924200117588 validation: 0.8003043532371521 | validation_accuracy: 76.88888888888889% \n",
      "EPOCH30:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 178\u001b[0m\n\u001b[0;32m    172\u001b[0m   plt\u001b[38;5;241m.\u001b[39mgcf()\u001b[38;5;241m.\u001b[39mset_size_inches(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m    174\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m model_path\n\u001b[1;32m--> 178\u001b[0m model_path\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43maugmented_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 128\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, validation_loader, loss_function, optimizer, EPOCHS)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    127\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 128\u001b[0m train_loss\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m##train for each epoch\u001b[39;00m\n\u001b[0;32m    130\u001b[0m running_validation_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    132\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[1;32mIn[1], line 105\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, epoch_index, loader, loss_function, optimizer)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loader):\n\u001b[0;32m    103\u001b[0m   inputs,labels\u001b[38;5;241m=\u001b[39mdata \u001b[38;5;66;03m#get the minibatch\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m   outputs\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#forward pass\u001b[39;00m\n\u001b[0;32m    107\u001b[0m   loss\u001b[38;5;241m=\u001b[39mloss_function(outputs,labels) \u001b[38;5;66;03m#compute loss\u001b[39;00m\n\u001b[0;32m    108\u001b[0m   running_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;66;03m#sum up the loss for the minibatches processed so far\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\alexnet.py:48\u001b[0m, in \u001b[0;36mAlexNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m---> 48\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[0;32m     50\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,ConcatDataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import models\n",
    "from torchvision.models.alexnet import AlexNet_Weights\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageOps\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms, models\n",
    "\n",
    "train_path = '/Users/Gaia/Desktop/CVPR-project/CVPR-project/data/train'\n",
    "test_path = '/Users/Gaia/Desktop/CVPR-project/CVPR-project/data/test'\n",
    "\n",
    "def transformation_for_AlexNet(img):\n",
    "  resize = transforms.Compose([transforms.Resize([224,224]),\n",
    "  transforms.ToTensor()])\n",
    "  i = resize(img)\n",
    "  return i\n",
    "\n",
    "\n",
    "train=ImageFolder(root=train_path,transform=transformation_for_AlexNet)\n",
    "test=ImageFolder(root=test_path,transform=transformation_for_AlexNet)\n",
    "\n",
    "\n",
    "#split training set into training and validation\n",
    "train_size=int(0.85*len(train))\n",
    "validation_size=len(train)-train_size\n",
    "training_set,validation_set=torch.utils.data.random_split(train,[train_size,validation_size])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TransformedDataSet():\n",
    "    \"\"\"Wrap a datset (created with imagefolder) to apply a transformation\"\"\"\n",
    "    def __init__(self, ds):\n",
    "        self.ds = ds\n",
    "        self.transformation=transforms.RandomHorizontalFlip(1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Get a sample from the dataset at the given index\"\"\"\n",
    "        img, label = self.ds[index]\n",
    "\n",
    "        # Apply the transformation if it is provided\n",
    "        if self.transformation:\n",
    "            img = self.transformation(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.ds)\n",
    "    \n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "augmented_training_set=TransformedDataSet(training_set)\n",
    "concatenated_dataset = torch.utils.data.ConcatDataset([training_set, augmented_training_set])\n",
    "augmented_train_loader = DataLoader(concatenated_dataset, batch_size=batch_size,shuffle=True,pin_memory=True)\n",
    "\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size,shuffle=False,pin_memory=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size,shuffle=False,pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "model=models.alexnet(weights=AlexNet_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "model.classifier[6]=nn.Linear(model.classifier[6].in_features,15)\n",
    "for param in model.classifier[6].parameters():\n",
    "    param.requires_grad=True\n",
    "\n",
    "\n",
    "\n",
    "loss_function=nn.CrossEntropyLoss()\n",
    "optimizer=optim.SGD(model.parameters(),lr=0.00001,momentum=0.9,weight_decay= 0.0005)\n",
    "\n",
    "\n",
    "def train_one_epoch(model,epoch_index,loader, loss_function,optimizer):\n",
    "  running_loss=0\n",
    "\n",
    "  for i, data in enumerate(loader):\n",
    "\n",
    "    inputs,labels=data #get the minibatch\n",
    "\n",
    "    outputs=model(inputs) #forward pass\n",
    "\n",
    "    loss=loss_function(outputs,labels) #compute loss\n",
    "    running_loss+=loss.item() #sum up the loss for the minibatches processed so far\n",
    "\n",
    "    optimizer.zero_grad() #reset gradients\n",
    "    loss.backward() #compute gradient\n",
    "    optimizer.step() #update weights\n",
    "\n",
    "  return running_loss/(i+1) # average loss per minibatch\n",
    "\n",
    "\n",
    "def train_model(model,train_loader,validation_loader,loss_function,optimizer,EPOCHS):\n",
    "  best_validation_loss=np.inf\n",
    "\n",
    "  train_losses = []\n",
    "  validation_losses = []\n",
    "  validation_accuracies = []\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "    print('EPOCH{}:'.format(epoch+1))\n",
    "\n",
    "    model.train(True)\n",
    "    train_loss=train_one_epoch(model,epoch,train_loader, loss_function,optimizer) ##train for each epoch\n",
    "\n",
    "    running_validation_loss=0.0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient computation and reduce memory consumption\n",
    "      correct=0\n",
    "      total=0\n",
    "      for i,vdata in enumerate(validation_loader):\n",
    "        vinputs,vlabels=vdata\n",
    "        voutputs=model(vinputs)\n",
    "        _,predicted=torch.max(voutputs.data,1)\n",
    "        vloss=loss_function(voutputs,vlabels)\n",
    "        running_validation_loss+=vloss\n",
    "        total+=vlabels.size(0)\n",
    "        correct+=(predicted==vlabels).sum().item()\n",
    "    validation_loss=running_validation_loss/(i+1)\n",
    "    validation_acc = 100*correct/total\n",
    "    print(f'LOSS train: {train_loss} validation: {validation_loss} | validation_accuracy: {validation_acc}% ')\n",
    "\n",
    "    if validation_loss<best_validation_loss: #save the model if it's the best so far\n",
    "      timestamp=datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "      best_validation_loss=validation_loss\n",
    "      model_path='model_{}_{}'.format(timestamp,epoch)\n",
    "      torch.save(model.state_dict(),model_path)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracies.append(validation_acc)\n",
    "\n",
    "\n",
    "  plt.plot(train_losses, color='tab:red', linewidth=3, label='train loss')\n",
    "  validation_losses_np = torch.stack(validation_losses).cpu().numpy() #move validation losses to cpu to plot with matplotlib\n",
    "  plt.plot(validation_losses_np, color='tab:green', linewidth=3, label='validation loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('CE loss')\n",
    "\n",
    "  ax_right = plt.gca().twinx()\n",
    "  #validation_accuracies_np = torch.stack(validation_accuracies).cpu().numpy() #move validation accuracies to cpu to plot with matplotlib\n",
    "  ax_right.plot(validation_accuracies, color='tab:green', linestyle='--', label='validation accuracy')\n",
    "  ax_right.set_ylabel('accuracy (%)')\n",
    "\n",
    "  plt.gcf().legend(ncol=3)\n",
    "  plt.gcf().set_size_inches(6, 3)\n",
    "\n",
    "  return model_path\n",
    "\n",
    "\n",
    "\n",
    "model_path=train_model(model,augmented_train_loader,validation_loader,loss_function,optimizer,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# Assuming you have a DataLoader for your dataset named `data_loader`\n",
    "# and the number of classes in your dataset is `num_classes`\n",
    "\n",
    "# Step 1: Load the pre-trained AlexNet model\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "\n",
    "# Step 2: Freeze all layers except the last fully connected layer\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Step 3: Modify the last fully connected layer for your task\n",
    "in_features = alexnet.classifier[6].in_features\n",
    "alexnet.classifier[6] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "# Step 4: Define loss function, optimizer, and train the modified model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(alexnet.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Train the model for a few epochs (you can adjust the number of epochs)\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = alexnet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "# (make sure to use a separate test set that was not seen during training)\n",
    "alexnet.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = alexnet(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
